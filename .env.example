# ============================================
# AI Pipeline Orchestrator - Configuration
# ============================================
#
# QUICK START:
#   1. Copy this file: cp .env.example .env
#   2. Choose a provider below and add your API key
#   3. Run the demo: npm run example:chat
#
# Get API Keys:
#   - Anthropic: https://console.anthropic.com
#   - OpenAI: https://platform.openai.com
#   - DeepSeek: https://platform.deepseek.com
#   - Ollama: https://ollama.com (free, local)
#

# Main AI Configuration (Required)
# ---------------------------------
# Used for chat responses in the demo
AI_PROVIDER=anthropic
AI_MODEL=claude-3-5-haiku-20241022

# Recommended Models:
#   anthropic â†’ claude-3-5-haiku-20241022 (fast, cheap)
#   anthropic â†’ claude-3-5-sonnet-20241022 (smart, balanced)
#   openai    â†’ gpt-4o-mini (fast, cheap)
#   openai    â†’ gpt-4o (smart, expensive)
#   deepseek  â†’ deepseek-chat (very cheap)
#   ollama    â†’ llama3.2:latest (free, local)
#   ollama    â†’ deepseek-r1:latest (free, local)

# Intent Classification (Optional)
# ---------------------------------
# By default, uses AI_PROVIDER/AI_MODEL for intent classification
# You can use a different (cheaper) model for classification to save costs
#
# ðŸ’¡ RECOMMENDED HYBRID SETUP (Best Cost/Performance):
# Use cloud provider for chat (best quality) + Ollama for intent (free)
#
# INTENT_PROVIDER=ollama
# INTENT_MODEL=deepseek-r1:latest
#
# Other hybrid examples:
# INTENT_PROVIDER=openai
# INTENT_MODEL=gpt-4o-mini

# Provider API Keys (Required)
# -----------------------------
# Uncomment and add the key for your chosen provider(s)

ANTHROPIC_API_KEY=your-anthropic-api-key-here
# OPENAI_API_KEY=your-openai-api-key-here
# DEEPSEEK_API_KEY=your-deepseek-api-key-here

# Provider Base URLs (Optional)
# ------------------------------
# DEEPSEEK_BASE_URL=https://api.deepseek.com

# Ollama Base URL (Required if using Ollama)
# Make sure Ollama is running: ollama serve
# Note: Do NOT include /api - the official client adds it automatically
OLLAMA_BASE_URL=http://localhost:11434

# Environment (Optional)
# ----------------------
# NODE_ENV=development
